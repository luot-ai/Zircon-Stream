# Zircon2024开发心得与思考

## 2025-01-31

今天对DCache架构进行了思考。主要的成果如下：

1. True Dual BRAM实现了store提交的“超车”

2. DCache的违例情况：

    - uncache store：需要保证之前的所有读写都完成，且之后的所有读操作都不能执行；
        - 所有uncache写会停留在c1s3中，直至**c2s3中该指令的rob_idx出现**。
    - uncache load：需要保证之前的所有读写都完成
        - 保证之前的读的顺序：乱序发射出的load会被强制送回发射队列重新执行
        - 保证之前的写的顺序：所有uncache读会停留在c1s3中，直至**该指令成为ROB中最后一条指令，且Store Buffer所有待提交指令全部完成执行**。

## 2025-02-01

今天主要的开发内容就是Store Buffer（SB）的实现。相较于上一版Ziron，这次的SB实现主要有以下几个主要改进：

1. 添加了rptr（ready pointer），用于指示SB中已经验证为可提交的存储请求；添加了cptr（commit pointer），用于指示SB中已经完成提交写入的存储请求，这个指针的更新信息由DCache提供。有了这两个指针，就可以通过二者差值获取到SB是否将所有已被验证的存储请求都提交了。

2. 在1的基础上，现在计划ROB提交特权指令时会等待所有SB中的存储请求都完成提交，再执行一次流水线flush。这次flush会清空SB所有表项，用来避免使用虚拟地址索引SB的冲突情况。

3. load访问SB时，这里不再基于tail指针重排SB表项，因为这样会导致电路面积的大量浪费（新表中每一个表项都是一个Mux1H的开销）。通过实现左右循环移位函数，我将比较结果先基于tail指针右移，通过独热优先编码器编码后，再左移，之后对SB中的命中表项进行选择。


## 2025-02-03

1. 将uncache write停留在c1s3的停止条件修改为：c2s3中该指令在SB中的位置出现。为此，SB将会把写入的表项号向后传递一级
2. 思考了一个问题：到底应该把SB放在DCache FSM级，还是DCache Data级。
    - 对于前者，可以直接使用物理地址进行索引，且store提交时也可以使用物理地址，但全相连查找的时序加上时序，加上FSM对输出数据信号的多选，加上输出信号的拼接，很可能成为一个很大的问题。
    - 对于后者，只能使用虚拟地址进行索引，且在FSM级获得到物理地址后，还需要写入到对应的SB表项中，而且由于查找时序，FSM级还需要一个额外的缓存项，假设store后面紧跟一条load，就需要利用此缓存项来判断load是否需要读取最新的写入数据。与此同时，SB每一项的存储成本也会加大，而且进程切换还需要等待所有写入都完成。
    - 综合考虑，将SB放在DCache FSM级。一个备用的解决方案是，将对写回数据是从Cache还是SB中读取的判断延迟到WB级——这并不会导致前递逻辑过于冗长。
3. 对于load对load和store的唤醒，只有当一条load到达DCache Data段时，才可以对后续的load和store执行唤醒。由于单流水线阻塞，这个唤醒不需要推测。为了功能完整性，我决定将DCache拓展一级，这一级专门用于对读出数据进行选择，选择后的数据会直接接入前递和写回逻辑中。
    - 注意，这一级和前一级的段间寄存器在miss时需要冲刷。

## 2025-02-04

今天构思了DCache和L2Cache的接口，并实现了DCache的主状态机。

1. DCache的结构与之前相同，两条通道，一条用于读和写第一次进入，另一条用于写提交。
2. 提交通道暂定为不使用状态机，使得DCache采用了写直达、写不分配的策略。读写通道只会向L2请求并读取数据，不会写入任何数据。
3. 采用写不分配的策略的主要原因有：简化DCache状态机设计、减少写提交通道的写入阻塞（只需要将写请求提交出去就好，流水线不再关心是否写完，这个写完确认只会影响SB的的cptr指针）。
4. 为了保证L2中强序非缓存读写的顺序，依然保持了原有的ICache和DCache各使用一个通道的设计（如果单独使用一个通道负责DCache写穿透，那么其实会导致读通道阻塞时写通道难以控制基本顺序）。特别需要注意的是，DCache两个通道不能同时发起对L2的访问，也就是说**L2的DCache通道同时只需要处理写或读即可**。

## 2025-02-05

今天实现了DCache的第一个通道，对一些问题又进行了改进

1. uncache write处理：之前的方案里，如果把请求停在c1s3，那么这个指令也不会通知ROB，导致ROB不会正常提交，SB中该请求也不会被提交，因此c2s3不会出现这个SB的idx。所以我将停止条件修改为：SB中所有表项全部提交完成。

2. 为了适应1的修改，SB的clear信号改为cptr和tptr来确定。由于SB在s3，当miss时不会写入，所以一定能等到所有的写请求全部提交完毕。

3. L2的uncache访问在生成读取数据时有问题，已做修改。

## 2025-02-06

今天主要是完成了DCache代码的编写，并解决了几个潜在的问题：

1. 当DCache接收到flush信号时，DCache各个阶段的寄存器都会被清空。如果此时正处于缺失处理，那么会等待完成本次处理（为了适应AXI总线访存的不可中断）；若正处于uncache的等待状态，则直接让状态机回归IDLE并清空miss信号。

## 2025-02-08

今天完成了DCache和L2Cache系统的整体测试，已经基本能够确认通过随机访问测试。

1. L2Cache中，dcache的hazard现在修改为，只要c2s3中存在写请求，那么dcache的hazard就为真。

2. DCacheFSM的重要修改：对L2的访问只能在L2可接受时持续一个周期，因此在外部增加了取边沿。

3. 所有DCache读通道的缺失请求必须等待SB中所有写请求完成，才能继续。这样做有两个原因，其一，避免了写提交通道在读缺失时向被替换后的行写入数据，使得恰好在命中时将数据写入了错误的行；其二，避免了读L2时写同一行导致换回的数据不及时。

4. 将SB满连接入阻塞体系，相应的修改了各个段间寄存器的执行逻辑

5. SB的指针和full信号生成做了重大修改：现在full不会再由hptr决定，而是由cptr和tptr决定。同时，之前SB查找逻辑的优先级有问题，已作出修改。

6. 修改了SB读命中逻辑和指针增长方向，用来适应PriorityEncoderOH的输出方式。

## 2025-02-10

1. 更新了DCache在load乱序不可缓存发射时miss寄存器的更新情况，并精简了状态机。

2. 提升了DCache在随机访存过程中的性能，现在非uncache的访问不会等待SB全部提交，而是在处理前锁定SB，在等待L2Cache返回时，DCache中所有残留的写请求必定都会提交进入L2Cache。

3. 进一步丰富了返回缓存rbuf的功能，现在当一次缺失处理开始时，rbuf会记录那些与第一通道缺失的读请求在同一行中的写请求，并将其写入数据及时写入rbuf。L2返回数据后，这些被写入的位置不会被覆盖，从而实现了数据及时重填入L1Cache。

4. 借助Claude修改了测试生成代码，使用了共享Random对象和Array进行改进，大幅度提升了生成的性能。

## 2025-02-16

1. 修复了SB的重要Bug：当SB被lock的时候，eptyn应当保持不变，否则可能会导致满时判空。

2. ICache重要更新：为了保证每个周期都能给出nfetch条指令，ICache现在每一行都会额外存储nfetch-1条指令。为了能够跨行获取到这些数据，ICache每次缺失都会向L2Cache连续请求两次，请求地址为连续的两次行地址。在较大概率下，这样只会引发多一个周期的等待。而对于L2Cache也需要跨行的情况，这样也不失为一种合理的预取。

3. Cache测试环境中，修复了AXI总线写入数据的问题，也将每个Memory Item中的Data改为Long型。

## 2025-02-17
今天主要是L2Cache的更新：

1. 在ICache和DCache同时都需要访问的地址范围内，我剥夺了ICache通道向主存写入数据的权利。现在，如果ICache先取得了这一块数据，当DCache再访问这一块数据时，会无效掉ICache请求的这一块，并将数据重新取入DCache所管辖的存储器。这样一来，ICache管辖的存储器内就不会有脏数据，同时也避免了**ICache需要写回一个块的过程中DCache再次写入这个块，导致ICache重填时覆盖掉写入结果**的情况。

2. 在以上改动的基础上，由于ICache不会写回，所以AXI-Arbiter的写状态机减少了一半的状态。同时由于ICache数据一定不脏，脏位表存储也减少了一半。

3. 现在的L2Cache状态机，每个通道状态机只能控制自己存储器的写使能，而不能控制其他存储器。但DCache通道的状态机可以将ICache通道的vld表项无效化。

## 2025-02-18

今天主要思考了译码逻辑的问题。

1. 为了进一步缩短前端流水级，我计划在ICache最后一级数据出来后直接使用尽可能精简的逻辑来译码出rj、rk、rd和各自的valid信号。根据我的观察，ListLookup并不可靠，因为里面有大量的迭代运算——这会导致时序爆炸。因此，我直接观察指令集各个指令的编码，用我能想到的最简方法来获取到各个值。同时，我还会保证，如果rx是0，那么rx_vld一定是0。同样地，如果rx_vld是0，那么rx一定是0。

2. 在译码逻辑中，我主要设计了op和imm的生成逻辑。不同于以往设计，本次我将br_type和mem_type也都复用到了op中。对于不同指令，其各个位的意义不同。
    - alu指令：\[5\]位是src1来源，\[4\]位是src2来源，\[3:0\]位是alu操作。
    - br指令：\[4:0\]位是跳转类型。
    - mem指令： \[5\]位表示存储，\[4\]位表示加载，\[3\]位表示是否是原子操作，\[2\]位表示零拓展，\[1:0\]位表示访存长度。

## 2025-02-25

今天设计了ROB和物理寄存器空闲列表，并修改了重命名相关的内容。

1. ROB复用了Cluster Index FIFO的结构，但是为了减少写入的组合逻辑，我重新修改了Index FIFO的写入逻辑，当类型是ROB表项时，入队逻辑只会写入前端的数据域，写入逻辑只会写入后端的数据域。

2. 物理寄存器空闲列表依然复用了Cluster Index FIFO的结构，不同的是，入队和出队的请求并不是压缩的（也就是说，可能入队端口1号和3号请求入队，但2号没有请求入队）。所以在分配物理寄存器时，我使用独热码创建了一个映射表，通过转置来完成双射，从而实现了复用。同时，我也检查出了之前在发射队列的空闲列表中存在的入队逻辑问题，并总结出了一个规律：非压缩一端到队列的映射是容易确定的，而队列出入端口到非压缩一端的映射只能通过矩阵转置来实现。

3. 重命名阶段，我重新修改了SRat：既然我们只会在提交阶段维护一个完全正确的映射表，我们为什么还要大费周章的复用端口来复原数据呢？我们完全可以一个周期把ARat中所有数据都福源道SRat中——这只是简单的寄存器复制而已。

## 2025-03-15

近期完成了SRT2除法器的设计。除去从各大网站上学习的内容之外，我还发现了一些其他问题：

1. SRT2除法中，一共需要除数前导零数减去被除数前导零数个周期，但若被除数前导零数大于除数前导零数，则直接结束。
2. SRT2除法需要将先除数左移直至最高位是1为止，被除数需要左移到次高位是1为止（具体实现应该是，被除数先左移到最高位是1，然后右移一位）。但是如果被除数前导零个数大于除数前导零个数，则需要将被除数左移除数前导零个数，这是因为这种情况余数就等于被除数，此时恰好能把结果设为被除数本身。
3. 还需要思考一种特殊情况：被除数和除数的前导零个数相同。这是有两种情况：被除数大于等于除数时，仍然需要进行一步运算（减法），才能获得余数。被除数小于除数时，余数就等于被除数。为了减少硬件复杂度，我将这两种情况合并，而第2点的处理方法恰好能处理这种问题（当被除数和除数前导零个数相同时，被除数左移后仍需要一个周期做减法，但此时余数是负数，后续电路会将这个余数加回来）。

## 2025-03-30

今天搭建完了Difftest环境，开始了测试。不过测试遇到了很多问题：

1. 使用next pc来查找ICache会遇到一个问题：如果ICache miss时，flush信号来了，那么PC会直接被冲刷，导致next pc会变成这个冲刷的PC的next pc，导致丢掉一组指令。我采用的解决方案是，冲刷时也会同步更新掉ICache第一、二流水级间的段间寄存器，同时如果此时恰好处于wait状态，那么addr_1H也需要选择第一级流水，保证地址的正确性。

2. 唤醒逻辑有误：我们需要在Rename阶段判断组内的RAW问题，并结合后续的Ready Board来判断是否入队就唤醒。不能只依靠唤醒总线来判断（一个好玩且搞笑的事情是，一看波型竟然发现Ready Board的ready信号没了，那Ready Board还有什么用呢？）

## 2025-04-01

今天成功通过了add测试，一定程度上验证了计算单元的正确性。下面记录一下Bug的由来和修改方式：

1. op位问题：只通过op\[6\]来判断是否为store并不准确，因为这一位在op当做ALU操作码时记录了src2的来源。因此，在写入ROB的时候，还需要通过func来判断。

2. 流水线停顿信号问题：Rename空闲列表为空时，应该阻止这一级指令向后流动；ROB满时，应该阻止发射队列写入新指令。

3. PC提交重定向问题：相比较之前的PC更新策略，新设计更注重加法器的复用，特别是针对在提交段不跳转导致PC+4的指令，Commit段从前会用加法器加好送到PC，现在直接复用PC顺序+4的加法器，通过修改其源操作数来完成重定向。

4. ICache问题：Commit段flush时，ICache的rreq同样应该被置为有效，因为NPC此时已经产生，故需要立即重新取指；同时，如果ICache处于miss状态且flush信号有效，由于ICache不可中断，故必须强行更新s1和s2段间寄存器。这里还有一些细节问题：
   - FSM的wait状态，这里会重新定向bram的读取地址，但如果此时flush有效，那么下一个周期读取的数据应该从s1读取，而不是常规情况下的s2。
   - FSM的idke状态，如果stall时flush有效，那么应该读取s1阶段的地址，而不是常规情况下的s2。

5. PReg\_Free\_List冲刷问题：在之前的设计中，重命名映射表所有项均为0，所以初期的提交需要屏蔽掉那些pprd为0的指令。但这样的话，我们就需要额外的一个指针用来恢复head指针（因为tail指针在初期遇到pprd为0的指令不会自增，而完全head只要提交了一条rd有效的指令就应该自增，一旦使用tail恢复，那么就会出现重新分配一些已经提交的物理寄存器的情况）。所以我采用了一种新的方法：为重命名表赋予初值（第i个寄存器重命名到物理寄存器i），且让空闲列表的表项中不含这些初值。这样既能节省了空闲列表的表项（现在仅需npreg-32个），还能够使用tail来恢复head指针，而且不再需要再提交时判断pprd是否为0（因为一定不会为0）。

## 2025-04-07

今天通过了除了乘除法、load store指令之外，其余全部指令的功能测试。

1. Index FIFO修改：原先的FIFO当flush时，对于ROB来讲，所有表项的complete都会归零。这在一定程度上增加了面积。现在的FIFO当flush时，对于非free list来讲，tptr会被复位到hptr位置（其实这并不会影响complete的问题，因为每次一定是入队的时候tptr把complete写成0），并取消了complete复位逻辑。

2. Dispatch模块中ROB入队逻辑没有考虑后端阻塞时不该写入rob的问题。

3. Branch模块没有考虑好对预测信息的处理。我尝试使用跳转偏移的offset和立即数来判断是否预测成功，但是对于预测不跳的情况，我希望预测器和预译码器能够将offset设为4，所以这里Branch对offset偏移的判断进行了修改。同时，为了减少硬件开销，这里使用块间进位并行加法器替代了原有的比较逻辑。

4. 预译码器现在会更新预测的信息了。

5. ROB的提交逻辑有问题：只有当deq.valid为真时，才会提交指令，原先只会看complete。

除此之外，今天在chiseltest的编译速度上有了极大提升：import chiseltest.internal.CachingAnnotation。

## 2025-04-09

今天结合functest，做出了很多重要修改：

1. 在空闲列表（发射队列、重命名）某一个队列为空时，应当阻止空闲表项的分配。因为一旦允许分配，如果此时进入的指令包数量大于剩余空闲项数量，那么就会导致多出来的指令包写入到了已经分配的表项，导致执行错误。

2. 乘除法流水线依旧需要前递，这是因为Ready Board目前只有一个，这使得每一个队列的唤醒信号不能相隔超过1个流水级，否则就会导致Ready Board中被唤醒但是乘除法的唤醒总线上并没有唤醒信息，导致乘除法指令被错误的提前执行。当然，这个问题也可以通过为每个发射队列配备不同的Ready Board来解决，但这样会为了一个理论上并不乐观的时序，损失IPC和很多硬件面积。

3. load-store流水线自我唤醒不能用rf段的唤醒信号，因为load-store流水线在RF段就需要使用Regfile的读取数据进行计算，因此只有写优先才有意义，其余的前递逻辑都无用。

4. 当唤醒源的指令包中lpv不为0，且此时Replay总线使能有效，那么这个唤醒源应当被禁用，因为它马上也要重新发射，这时它唤醒的指令包的lpv左移可能为0，但这个指令包不该被发射。

5. DCache在遇到提交段的flush时，需要考虑如下问题：
   - 保护c1s3寄存器：如果此时处于miss状态，那么c1s3寄存器不可被清空，因为AXI总线事务不能被中断，这个寄存器需要负责发射请求。但如果此时并没有处于miss状态，那么这个寄存器应当被清空。
   - miss寄存器：和c1s3寄存器一样，如果此时处于miss状态，那么miss寄存器不可被清空，因为需要等待L2返回数据。
   - c1s2寄存器应当被立刻清空。
   - c1s3对Store Buffer的写入应当被禁止。

6. Store Buffer需要添加一个commit位，用来标记当前表项是被确认要提交的，从而在flush时避免被“误刷”。这个位由rptr来维护。

## 2025-04-11

今天通过了所有functest，并重新规范了变量函数命名方法

1. 事实上乘法现在在EX3唤醒，完全浪费了一个周期，因此改到EX2唤醒（虽然这样依然会浪费掉前递逻辑）
   
2. Dispatcher中，如果一个发射队列已满，那么所有的入队valid信号都应该无效，以保证一个队列不会被重复写入指令。
   
3. DCache中仍然存在一个隐含的问题：当读缺失时，我以为锁住SB不允许提交就能防止返回的旧数据写到DCache中，但事实上，recursion测试在写DCache的那一个周期往rbuf里提交了一条数据，这是因为在锁住之前，SB之前提交的一个写一直在等待，而读缺失数据写回DCache后，那个与这条相关的写操作刚刚发起写入（因为现在是读缺失优先），导致rbuf没有将最新获得的写数据写入DCache。所以，现在FSM会在refill阶段等待所有写请求结束，才会将数据写入DCache。当然，这里还存在优化的可能性：既然你都要等了，为什么不优先写缺失处理呢？
   
4. 非最旧load缺失时无需hold，直接处理即可。

5. 进一步细化了wakeup包生成的逻辑，使其能够支持访存流水线不同阶段的唤醒。

## 2025-04-12

以后Java一定要给我规范命名！！

变量名小写字母开头，函数名大写字母开头，驼峰式，有缩写单词全大写！

## 2025-04-16

整理一下近期做过的所有改动：

1. Dispatch存在问题：如果ROB满，那么所有发射队列的入队valid为无效，但原来是有效的。

2. DCache中，如果sb full时卡在c1s3一条写，那么一旦flush，这条写不会被冲刷，因此需要能刷掉c1s3的逻辑。但是，如果是因为dcache miss（uncache write）导致卡在c1s3，那么flush时应该保留这条写。我暂时是，对于uncache write我就不认为它miss，这样暂时无法保证所有非可缓存读写都是强序。

3. 除法和后续一条已经到rf段的除法相关时需要注意，如果在busy时replay来了，不能无脑更新rf寄存器，因为此时这条指令可能不是由访存唤醒的。因此，此处我打了一个补丁，这时需要看rf段的指令是否和replay的指令相关，如果相关，才更新rf寄存器。有点丑陋，但能用就行。这里其实有一个事实：只有除法流水线的rf段才会出现这个问题。

4. SB在面临冲刷时，fulln和allclear寄存器都需要根据当前队列中的已有信息进行更新，而不能通过那些指针来更新。

## 2025-04-21

今天调试完成了分支预测，CoreMark IPC达到了1.022349！

Y P在论文里提出了一种同时预测多个分支的策略，我试图尝试，但是很显然，那时序一定会爆炸（就是使用第一条指令的预测结果来选第二条，同时你还需要决策一组指令里谁是分支指令，要“向右压缩1”）。其实，一组指令最大的预测问题就是能否预测组内多个分支的情况。这需要基于以下几个事实：

1. 若组内某个指令跳转，则其之后的所有指令都不可能再有效，遑论跳转。
2. 本次全局历史需要移动的位数等于跳转的分支前有几条分支指令（包括自己）。
3. 全局历史高位需要补的内容是1左移需要移动的位数。

为了解决组内跳转的问题，我还使用了一种组内局部预测的办法。简单来说，使用一个PHT来预测组内每个指令是否跳转。若更新时发现其跳转，那么其饱和加一，之前的指令饱和减一。

## 2025-05-03

今天对提交段做了几个很重要的改进：

1. 原先的设计中，由于我对于Chisel的语法不够熟练，导致了Cluster Index FIFO每次写或者入队只能整个项更新，而不是能选择其中几个成员进行更新。现在我会在每次更新时，检测这个类是否含有enqueue和write方法，如果有，则可以调用这些定制化的方法来进行写入，否则只能使用默认的全部更新。

2. 跳转指令一般只会占据总程序的30%，但现在ROB中每一个表项都会有一个32位宽的成员，用来存放分支的跳转结果，以方便在提交时进行更新和冲刷——这显然是一个浪费的行为。因此，我提出了Branch Data Buffer（BDB）。它的长度只有ROB的30%，但可以存储所有进入ROB中的分支指令的跳转情况、跳转目标等信息，这样大大减少了ROB的面积。

3. 精简发射队列：现在在RF段，算数流水线会使用指令的robIdx和bdbIdx来从ROB和BDB中获取跳转预测偏移量和PC，这两个信息已经无需再在发射队列里存储了。

## 2025-05-07

今天遇到了一个隐藏的很深的bug。

当commit段发出冲刷信号时，发射队列里所有的表项的instExi成员都会立刻被置为0。但是，回收可用表项是是需要很多周期的（因为要一个一个的写回到空闲列表里）。因此，如果冲刷后流水线立刻又取出一条访存指令，且在剩余的队列项全部被回收之前就触发了DCache的miss，那么之前还没有被回收的表项就有可能被错误重演，导致了访存指令的错误执行。

解决这个问题最简单的方法就是，减少发射队列的表项，使其少于流水线总长度——这显然会降低并行发掘能力。还有一种方法就是，当flush信号到来时，将发射队列的一个计数器置位，每个周期减一，归零后允许继续插入发射队列。

## 2025-09-07

这段时间一直在基于Vivado的综合来提升时序，目前已经从最初的40MHz提升到了100MHz，整个过程还是比较值得记录的。

1. Block Memory相关：BRAM的资源还是比较紧张的，我把所有Cache的容量都缩小了；Block Memory用Vivado的模板是错的，那个是先读然后锁结果，时序比较烂，用我现在文件里的模板是先锁地址，再读，就比较不错。
2. 除法器的预处理开销比较大，最好分两个周期分别计算取反和绝对值。
3. 发射仲裁逻辑中，如果能加一位，把两个源寄存器各自的就绪信号或起来更新到这一位，在发射时就不需要prjWk && prkWk了，减少了一层逻辑，包括load-store队列的stBefore逻辑也可以合并到这一位。同时，年龄比较逻辑也做出了更新。
4. 预译码逻辑：对Branch指令进行地址修复是意义不大的。
5. Mux1H(PriorityEncoderOH(x))的逻辑是蠢的，使用PriorityMux来代替，因为前面这种计算相当于完成了两次log级别的计算。这也启示我们，如果多选独热码的生成复杂度不为O(1)，那么Mux1H一定有上位代替。
6. Decode Width为3或者2在coremark程序上没有明显的区别。
7. BTB和RAS还是应该存储跳转的目标地址，避免读取后的加法。但在IF阶段，可以将这个值直接减去pc获得offset，方便预译码器直接根据立即数和offset的比较来确定预测是否正确。
8. 复用PC+4的加法器的思路是错的，因为有些开销比较大的计算，例如预译码，完全可以在这个加法器旁边并行执行，而无需再经历这个加法器的延迟。

## 2025-09-12

1. Bug修复：在执行Clang-22编译的FFT程序时，处理器出现过久未提交指令的情况。观察波型发现，由于错误的利用DCache的FSM生成的lock信号，导致Storbuffer的eptyn寄存器被错误地锁住，没有更新，而在此期间rob直接提交了4个确认写请求，eptyn没有更新，而lock结束时，由于SB表项是4，恰好rptr和hptr重叠了，SB认为此时没有待提交到L2的请求，于是卡死。但其中其实是有4个待提交的请求。（p.s. 感谢王越同学对Bug的报告）

## 2025-10-22

1. Bug修复：在执行Clang-22编译的FFT程序时，处理器出现过久未提交指令的情况。观察波型发现，由于错误的利用DCache的FSM生成的lock信号，导致Storbuffer的eptyn寄存器在lock时，eptyn会deq.valid的时候被错误的复原，导致后续认为没有待提交请求。所以我对sb的deq.valid加了条件，也就是当lock的时候不会给把valid信号送入sb。（p.s. 感谢王越同学对Bug的报告）


